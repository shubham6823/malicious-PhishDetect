import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import pickle
import datetime
import logging

# Configure logging
logging.basicConfig(filename='url_detection.log', level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

# Step 1: Load the dataset (replace with your dataset if available)
data = pd.read_csv('malicious_urls.csv')  # Assumes dataset contains 'URL' and 'Label' columns
logging.info("Dataset loaded successfully.")

# Step 2: Feature Extraction
def extract_features(urls):
    vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 4))  # Character n-grams
    features = vectorizer.fit_transform(urls)
    return vectorizer, features

print("Extracting features from URLs...")
logging.info("Extracting features from URLs.")
vectorizer, X = extract_features(data['URL'])
y = data['Label']

# Step 3: Split data into training and testing sets
print("Splitting dataset into training and testing...")
logging.info("Splitting dataset into training and testing.")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Train the model
print("Training the Random Forest Classifier...")
logging.info("Training the Random Forest Classifier.")
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Step 5: Evaluate the model
print("Evaluating the model...")
logging.info("Evaluating the model.")
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))
logging.info(f"Model evaluation completed with accuracy: {accuracy:.2f}")

# Step 6: Save the model and vectorizer
print("Saving the model and vectorizer...")
logging.info("Saving the model and vectorizer.")
with open('url_classifier.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)
with open('vectorizer.pkl', 'wb') as vec_file:
    pickle.dump(vectorizer, vec_file)

logging.info("Model and vectorizer saved successfully.")

# Step 7: Predict on new URLs
def predict_url(url):
    logging.info(f"Predicting for URL: {url}")
    with open('url_classifier.pkl', 'rb') as model_file:
        loaded_model = pickle.load(model_file)
    with open('vectorizer.pkl', 'rb') as vec_file:
        loaded_vectorizer = pickle.load(vec_file)

    url_features = loaded_vectorizer.transform([url])
    prediction = loaded_model.predict(url_features)
    logging.info(f"Prediction result for URL '{url}': {prediction[0]}")
    return prediction

# Step 8: Threat Analysis (Log and Monitor)
def log_threat_analysis(url, prediction):
    status = "Malicious" if prediction == 1 else "Benign"
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    with open('threat_analysis.log', 'a') as threat_log:
        threat_log.write(f"{timestamp}, {url}, {status}\n")
    logging.info(f"Threat analysis logged for URL '{url}' with status: {status}.")

# Test the prediction function
sample_url = "http://example-phishing-site.com"
prediction = predict_url(sample_url)[0]
log_threat_analysis(sample_url, prediction)
print(f"Prediction for '{sample_url}':", prediction)
